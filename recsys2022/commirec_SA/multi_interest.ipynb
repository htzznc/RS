{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import faiss\n",
    "from collections import defaultdict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'embedding_dim' : 64,\n",
    "    'lr':0.001,\n",
    "    'batch_size':256,\n",
    "    'Epoch':100,\n",
    "    'max_length' : 20,\n",
    "    'device' : torch.device('cuda'),\n",
    "    'K':4,\n",
    "    'n_items':15406,\n",
    "    'train_path':'train_enc.csv',\n",
    "    \"val_path\": 'valid_enc.csv',\n",
    "    \"test_path\":'test_enc.csv'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config['train_path'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15405"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['item_id'].nunique()\n",
    "# df.groupby('user_id')['item_id'].apply(list).to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqnenceDataset(Dataset):\n",
    "    def __init__(self, config, df, phase='train'):\n",
    "        self.config = config\n",
    "        self.df = df\n",
    "        self.max_length = self.config['max_length']\n",
    "        self.df = self.df.sort_values(by=['user_id', 'timestamp'])\n",
    "        self.user2item = self.df.groupby('user_id')['item_id'].apply(list).to_dict()\n",
    "        self.user_list = self.df['user_id'].unique()\n",
    "        self.phase = phase\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return len(self.user2item)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.phase == 'train':\n",
    "            user_id = self.user_list[index]\n",
    "            item_list = self.user2item[user_id]\n",
    "            hist_item_list = []\n",
    "            hist_mask_list = []\n",
    "\n",
    "            k = np.random.randint(4,len(item_list))\n",
    "            item_id = item_list[k]  # 该index对应的item加入item_id_list\n",
    "\n",
    "            if k >= self.max_length:  # 选取seq_len个物品\n",
    "                hist_item_list.append(item_list[k - self.max_length: k])\n",
    "                hist_mask_list.append([1.0] * self.max_length)\n",
    "            else:\n",
    "                hist_item_list.append(item_list[:k] + [0] * (self.max_length - k))\n",
    "                hist_mask_list.append([1.0] * k + [0.0] * (self.max_length - k))\n",
    "\n",
    "            return torch.Tensor(hist_item_list).squeeze(0).long(), torch.Tensor(hist_mask_list).squeeze(\n",
    "                0).long(), torch.Tensor([item_id]).long()\n",
    "        else:\n",
    "            user_id = self.user_list[index]\n",
    "            item_list = self.user2item[user_id]\n",
    "            hist_item_list = []\n",
    "            hist_mask_list = []\n",
    "\n",
    "            k = int(0.8 * len(item_list))\n",
    "\n",
    "            if k >= self.max_length:  # 选取seq_len个物品\n",
    "                hist_item_list.append(item_list[k - self.max_length: k])\n",
    "                hist_mask_list.append([1.0] * self.max_length)\n",
    "            else:\n",
    "                hist_item_list.append(item_list[:k] + [0] * (self.max_length - k))\n",
    "                hist_mask_list.append([1.0] * k + [0.0] * (self.max_length - k))\n",
    "\n",
    "            return torch.Tensor(hist_item_list).squeeze(0).long(), torch.Tensor(hist_mask_list).squeeze(\n",
    "                0).long(), item_list[k:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    hist_item = torch.rand(len(batch), batch[0][0].shape[0])\n",
    "    hist_mask = torch.rand(len(batch), batch[0][0].shape[0])\n",
    "    item_list = []\n",
    "    for i in range(len(batch)):\n",
    "        hist_item[i,:] = batch[i][0]\n",
    "        hist_mask[i,:] = batch[i][1]\n",
    "        item_list.append(batch[i][2])\n",
    "    hist_item = hist_item.long()\n",
    "    hist_mask = hist_mask.long()\n",
    "    return hist_item, hist_mask, item_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = config['device']\n",
    "train_df = pd.read_csv(config['train_path'])\n",
    "valid_df = pd.read_csv(config['val_path'])\n",
    "test_df = pd.read_csv(config['test_path'])\n",
    "train_dataset = SeqnenceDataset(config, train_df, phase='train')\n",
    "valid_dataset = SeqnenceDataset(config, valid_df, phase='test')\n",
    "test_dataset = SeqnenceDataset(config, test_df, phase='test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(train_dataset,batch_size=config['batch_size'], shuffle=True)\n",
    "testloader = DataLoader(test_dataset,batch_size=config['batch_size'], collate_fn=my_collate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110120"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['user_id'].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(trainloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## commirec_SA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiInterest_SA(nn.Module):\n",
    "    def __init__(self,embedding_dim, K, d=None) -> None:\n",
    "        super(MultiInterest_SA, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.K = K\n",
    "        \n",
    "        self.d = d if d is not None else embedding_dim*4\n",
    "        self.W1 = torch.nn.parameter.Parameter(torch.rand(self.embedding_dim, self.d), requires_grad = True)\n",
    "        self.W2 = torch.nn.parameter.Parameter(torch.rand(self.d, self.K), requires_grad=True)\n",
    "        \n",
    "    def forward(self, seq_emb, mask = None):\n",
    "        # seq_embed.shape (batch_size, seq_len, embedding_dim) \n",
    "        H = torch.einsum('bse, ed -> bsd', seq_emb, self.W1).tanh()\n",
    "        if mask != None:\n",
    "            A = torch.einsum('bsd, dk -> bsk', H, self.W2) + -1.e9 * (1 - mask.float())\n",
    "            A = F.softmax(A, dim=1)\n",
    "        else:\n",
    "            A = F.softmax(torch.einsum('bsd, dk -> bsk', H, self.W2), dim=1)\n",
    "        A = A.permute(0, 2, 1)\n",
    "        multi_interest_emb = torch.matmul(A, seq_emb)\n",
    "        return multi_interest_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Comirec_SA(nn.Module):\n",
    "    def __init__(self,config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding_dim = self.config['embedding_dim']\n",
    "        self.max_length = self.config['max_length']\n",
    "        self.device = self.config['device']\n",
    "        self.n_items = self.config['n_items']\n",
    "        \n",
    "        self.item_emb = nn.Embedding(self.n_items, self.embedding_dim, padding_idx=0)\n",
    "        self.multi_interest_sa = MultiInterest_SA(self.embedding_dim, K= self.config['K'])\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "        self.reset_parameter()\n",
    "        \n",
    "    def output_items(self):\n",
    "        return self.item_emb.weight\n",
    "    \n",
    "    def calculate_score(self, user_emb):\n",
    "        all_items = self.item_emb.weight\n",
    "        scores = torch.matmul(user_emb, all_items.transpose(1, 0))  # [b, n]\n",
    "        return scores\n",
    "\n",
    "    def calculate_loss(self,user_emb,item):\n",
    "        scores = self.calculate_score(user_emb)\n",
    "        return self.loss_func(scores,item.squeeze(1))\n",
    "        \n",
    "    def reset_parameter(self):\n",
    "        for weight in self.parameters():\n",
    "            torch.nn.init.kaiming_normal_(weight)\n",
    "            \n",
    "    def forward(self, item_seq, mask, item, train = True):\n",
    "        if train:\n",
    "            seq_emb = self.item_emb(item_seq)\n",
    "            item_emb = self.item_emb(item).squeeze(1)\n",
    "            mask = mask.unsqueeze(-1).float()\n",
    "            multi_interest_emb = self.multi_interest_sa(seq_emb,mask)  # (batch_size, K, embedding_dim)\n",
    "            cos_res = torch.bmm(multi_interest_emb, item_emb.squeeze(1).unsqueeze(-1))\n",
    "            k_index = torch.argmax(cos_res, dim=1)\n",
    "\n",
    "            best_interest_emb = torch.rand(multi_interest_emb.shape[0], multi_interest_emb.shape[2]).to(self.device)\n",
    "            # 用每一条中 最好的  （概率最大的 cos_res最大的  兴趣点向量 去和item向量计算损失\n",
    "            for k in range(multi_interest_emb.shape[0]):\n",
    "                best_interest_emb[k, :] = multi_interest_emb[k, k_index[k], :]\n",
    "\n",
    "            loss = self.calculate_loss(best_interest_emb, item)\n",
    "            output_dict = {\n",
    "                'user_emb':multi_interest_emb,\n",
    "                'loss':loss,\n",
    "                'acc_50':-1\n",
    "            }\n",
    "            \n",
    "        else:\n",
    "            seq_emb = self.item_emb(item_seq)\n",
    "\n",
    "            mask = mask.unsqueeze(-1).float()\n",
    "            multi_interest_emb = self.multi_interest_sa(seq_emb, mask)\n",
    "            \n",
    "            output_dict = {\n",
    "                'user_emb': multi_interest_emb,\n",
    "            }\n",
    "        \n",
    "        return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Comirec_SA(config)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = config['lr'], betas=(0.5,0.99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch :1 Loss:  9.526:   2%|▏         | 9/431 [00:00<00:36, 11.46it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch :1 Loss:  8.259: 100%|██████████| 431/431 [00:31<00:00, 13.57it/s]\n",
      "Epoch :2 Loss:  7.049: 100%|██████████| 431/431 [00:32<00:00, 13.42it/s]\n",
      "Epoch :3 Loss:  6.788: 100%|██████████| 431/431 [00:31<00:00, 13.57it/s]\n",
      "Epoch :4 Loss:  6.605: 100%|██████████| 431/431 [00:30<00:00, 14.14it/s]\n",
      "Epoch :5 Loss:  6.448: 100%|██████████| 431/431 [00:29<00:00, 14.44it/s]\n",
      "Epoch :6 Loss:  6.337: 100%|██████████| 431/431 [00:30<00:00, 14.00it/s]\n",
      "Epoch :7 Loss:  6.232: 100%|██████████| 431/431 [00:31<00:00, 13.89it/s]\n",
      "Epoch :8 Loss:  6.165: 100%|██████████| 431/431 [00:30<00:00, 14.05it/s]\n",
      "Epoch :9 Loss:  6.112: 100%|██████████| 431/431 [00:30<00:00, 14.29it/s]\n",
      "Epoch :10 Loss:  6.061: 100%|██████████| 431/431 [00:31<00:00, 13.53it/s]\n",
      "Epoch :11 Loss:  6.018: 100%|██████████| 431/431 [00:32<00:00, 13.13it/s]\n",
      "Epoch :12 Loss:  5.984: 100%|██████████| 431/431 [00:29<00:00, 14.45it/s]\n",
      "Epoch :13 Loss:  5.954: 100%|██████████| 431/431 [00:31<00:00, 13.57it/s]\n",
      "Epoch :14 Loss:  5.920: 100%|██████████| 431/431 [00:30<00:00, 14.35it/s]\n",
      "Epoch :15 Loss:  5.903: 100%|██████████| 431/431 [00:29<00:00, 14.55it/s]\n",
      "Epoch :16 Loss:  5.886: 100%|██████████| 431/431 [00:31<00:00, 13.80it/s]\n",
      "Epoch :17 Loss:  5.864: 100%|██████████| 431/431 [00:31<00:00, 13.85it/s]\n",
      "Epoch :18 Loss:  5.849: 100%|██████████| 431/431 [00:29<00:00, 14.60it/s]\n",
      "Epoch :19 Loss:  5.836: 100%|██████████| 431/431 [00:30<00:00, 13.96it/s]\n",
      "Epoch :20 Loss:  5.821: 100%|██████████| 431/431 [00:30<00:00, 14.01it/s]\n",
      "Epoch :21 Loss:  5.806: 100%|██████████| 431/431 [00:29<00:00, 14.57it/s]\n",
      "Epoch :22 Loss:  5.798: 100%|██████████| 431/431 [00:31<00:00, 13.60it/s]\n",
      "Epoch :23 Loss:  5.794: 100%|██████████| 431/431 [00:29<00:00, 14.66it/s]\n",
      "Epoch :24 Loss:  5.764: 100%|██████████| 431/431 [00:30<00:00, 14.19it/s]\n",
      "Epoch :25 Loss:  5.761: 100%|██████████| 431/431 [00:30<00:00, 13.98it/s]\n",
      "Epoch :26 Loss:  5.761: 100%|██████████| 431/431 [00:30<00:00, 14.26it/s]\n",
      "Epoch :27 Loss:  5.755: 100%|██████████| 431/431 [00:30<00:00, 14.19it/s]\n",
      "Epoch :28 Loss:  5.738: 100%|██████████| 431/431 [00:29<00:00, 14.39it/s]\n",
      "Epoch :29 Loss:  5.730: 100%|██████████| 431/431 [00:32<00:00, 13.39it/s]\n",
      "Epoch :30 Loss:  5.726: 100%|██████████| 431/431 [00:31<00:00, 13.72it/s]\n",
      "Epoch :31 Loss:  5.724: 100%|██████████| 431/431 [00:30<00:00, 13.98it/s]\n",
      "Epoch :32 Loss:  5.719: 100%|██████████| 431/431 [00:29<00:00, 14.64it/s]\n",
      "Epoch :33 Loss:  5.711: 100%|██████████| 431/431 [00:31<00:00, 13.50it/s]\n",
      "Epoch :34 Loss:  5.702: 100%|██████████| 431/431 [00:31<00:00, 13.78it/s]\n",
      "Epoch :35 Loss:  5.702: 100%|██████████| 431/431 [00:29<00:00, 14.45it/s]\n",
      "Epoch :36 Loss:  5.689: 100%|██████████| 431/431 [00:31<00:00, 13.86it/s]\n",
      "Epoch :37 Loss:  5.694: 100%|██████████| 431/431 [00:28<00:00, 14.87it/s]\n",
      "Epoch :38 Loss:  5.677: 100%|██████████| 431/431 [00:30<00:00, 14.15it/s]\n",
      "Epoch :39 Loss:  5.678: 100%|██████████| 431/431 [00:32<00:00, 13.38it/s]\n",
      "Epoch :40 Loss:  5.678: 100%|██████████| 431/431 [00:29<00:00, 14.67it/s]\n",
      "Epoch :41 Loss:  5.674: 100%|██████████| 431/431 [00:30<00:00, 14.24it/s]\n",
      "Epoch :42 Loss:  5.658: 100%|██████████| 431/431 [00:30<00:00, 14.13it/s]\n",
      "Epoch :43 Loss:  5.652: 100%|██████████| 431/431 [00:30<00:00, 13.96it/s]\n",
      "Epoch :44 Loss:  5.651: 100%|██████████| 431/431 [00:31<00:00, 13.81it/s]\n",
      "Epoch :45 Loss:  5.651: 100%|██████████| 431/431 [00:30<00:00, 14.12it/s]\n",
      "Epoch :46 Loss:  5.644: 100%|██████████| 431/431 [00:31<00:00, 13.84it/s]\n",
      "Epoch :47 Loss:  5.642: 100%|██████████| 431/431 [00:30<00:00, 13.95it/s]\n",
      "Epoch :48 Loss:  5.641: 100%|██████████| 431/431 [00:30<00:00, 14.16it/s]\n",
      "Epoch :49 Loss:  5.638: 100%|██████████| 431/431 [00:30<00:00, 13.91it/s]\n",
      "Epoch :50 Loss:  5.628: 100%|██████████| 431/431 [00:31<00:00, 13.89it/s]\n",
      "Epoch :51 Loss:  5.636: 100%|██████████| 431/431 [00:30<00:00, 14.22it/s]\n",
      "Epoch :52 Loss:  5.624: 100%|██████████| 431/431 [00:29<00:00, 14.46it/s]\n",
      "Epoch :53 Loss:  5.628: 100%|██████████| 431/431 [00:30<00:00, 14.15it/s]\n",
      "Epoch :54 Loss:  5.620: 100%|██████████| 431/431 [00:31<00:00, 13.82it/s]\n",
      "Epoch :55 Loss:  5.616: 100%|██████████| 431/431 [00:31<00:00, 13.69it/s]\n",
      "Epoch :56 Loss:  5.620: 100%|██████████| 431/431 [00:30<00:00, 13.97it/s]\n",
      "Epoch :57 Loss:  5.621: 100%|██████████| 431/431 [00:31<00:00, 13.89it/s]\n",
      "Epoch :58 Loss:  5.602: 100%|██████████| 431/431 [00:31<00:00, 13.69it/s]\n",
      "Epoch :59 Loss:  5.599: 100%|██████████| 431/431 [00:30<00:00, 13.92it/s]\n",
      "Epoch :60 Loss:  5.612: 100%|██████████| 431/431 [00:30<00:00, 14.03it/s]\n",
      "Epoch :61 Loss:  5.599: 100%|██████████| 431/431 [00:30<00:00, 14.11it/s]\n",
      "Epoch :62 Loss:  5.594: 100%|██████████| 431/431 [00:31<00:00, 13.87it/s]\n",
      "Epoch :63 Loss:  5.601: 100%|██████████| 431/431 [00:31<00:00, 13.67it/s]\n",
      "Epoch :64 Loss:  5.596: 100%|██████████| 431/431 [00:28<00:00, 15.07it/s]\n",
      "Epoch :65 Loss:  5.586: 100%|██████████| 431/431 [00:27<00:00, 15.51it/s]\n",
      "Epoch :66 Loss:  5.596: 100%|██████████| 431/431 [00:30<00:00, 14.08it/s]\n",
      "Epoch :67 Loss:  5.589: 100%|██████████| 431/431 [00:29<00:00, 14.80it/s]\n",
      "Epoch :68 Loss:  5.590: 100%|██████████| 431/431 [00:30<00:00, 14.33it/s]\n",
      "Epoch :69 Loss:  5.587: 100%|██████████| 431/431 [00:30<00:00, 14.12it/s]\n",
      "Epoch :70 Loss:  5.584: 100%|██████████| 431/431 [00:30<00:00, 14.01it/s]\n",
      "Epoch :71 Loss:  5.585: 100%|██████████| 431/431 [00:30<00:00, 14.07it/s]\n",
      "Epoch :72 Loss:  5.580: 100%|██████████| 431/431 [00:30<00:00, 14.26it/s]\n",
      "Epoch :73 Loss:  5.581: 100%|██████████| 431/431 [00:30<00:00, 14.18it/s]\n",
      "Epoch :74 Loss:  5.575: 100%|██████████| 431/431 [00:29<00:00, 14.81it/s]\n",
      "Epoch :75 Loss:  5.578: 100%|██████████| 431/431 [00:29<00:00, 14.44it/s]\n",
      "Epoch :76 Loss:  5.576: 100%|██████████| 431/431 [00:30<00:00, 14.23it/s]\n",
      "Epoch :77 Loss:  5.567: 100%|██████████| 431/431 [00:30<00:00, 14.01it/s]\n",
      "Epoch :78 Loss:  5.574: 100%|██████████| 431/431 [00:30<00:00, 14.26it/s]\n",
      "Epoch :79 Loss:  5.568: 100%|██████████| 431/431 [00:30<00:00, 13.98it/s]\n",
      "Epoch :80 Loss:  5.566: 100%|██████████| 431/431 [00:30<00:00, 14.28it/s]\n",
      "Epoch :81 Loss:  5.555: 100%|██████████| 431/431 [00:30<00:00, 14.18it/s]\n",
      "Epoch :82 Loss:  5.560: 100%|██████████| 431/431 [00:28<00:00, 14.92it/s]\n",
      "Epoch :83 Loss:  5.559: 100%|██████████| 431/431 [00:30<00:00, 14.29it/s]\n",
      "Epoch :84 Loss:  5.561: 100%|██████████| 431/431 [00:30<00:00, 14.26it/s]\n",
      "Epoch :85 Loss:  5.560: 100%|██████████| 431/431 [00:30<00:00, 14.26it/s]\n",
      "Epoch :86 Loss:  5.566: 100%|██████████| 431/431 [00:30<00:00, 14.19it/s]\n",
      "Epoch :87 Loss:  5.551: 100%|██████████| 431/431 [00:31<00:00, 13.78it/s]\n",
      "Epoch :88 Loss:  5.563: 100%|██████████| 431/431 [00:30<00:00, 14.19it/s]\n",
      "Epoch :89 Loss:  5.549: 100%|██████████| 431/431 [00:29<00:00, 14.49it/s]\n",
      "Epoch :90 Loss:  5.560: 100%|██████████| 431/431 [00:29<00:00, 14.52it/s]\n",
      "Epoch :91 Loss:  5.556: 100%|██████████| 431/431 [00:30<00:00, 14.09it/s]\n",
      "Epoch :92 Loss:  5.553: 100%|██████████| 431/431 [00:30<00:00, 14.08it/s]\n",
      "Epoch :93 Loss:  5.552: 100%|██████████| 431/431 [00:30<00:00, 14.03it/s]\n",
      "Epoch :94 Loss:  5.547: 100%|██████████| 431/431 [00:30<00:00, 14.05it/s]\n",
      "Epoch :95 Loss:  5.559: 100%|██████████| 431/431 [00:30<00:00, 14.06it/s]\n",
      "Epoch :96 Loss:  5.548: 100%|██████████| 431/431 [00:30<00:00, 14.02it/s]\n",
      "Epoch :97 Loss:  5.544: 100%|██████████| 431/431 [00:30<00:00, 14.08it/s]\n",
      "Epoch :98 Loss:  5.531: 100%|██████████| 431/431 [00:29<00:00, 14.49it/s]\n",
      "Epoch :99 Loss:  5.542: 100%|██████████| 431/431 [00:30<00:00, 14.10it/s]\n",
      "Epoch :100 Loss:  5.546: 100%|██████████| 431/431 [00:30<00:00, 14.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 1 + config['Epoch']):\n",
    "    pbar = tqdm(trainloader)\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for batch_data in pbar:\n",
    "        (item_seq, mask, item) = batch_data\n",
    "        item_seq = item_seq.to(device)\n",
    "        mask = mask.to(device)\n",
    "        item = item.to(device)\n",
    "\n",
    "        output_dict = model(item_seq, mask, item)\n",
    "        loss = output_dict['loss']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        pbar.set_description('Epoch :{} Loss:{:7.3f}'.format(epoch, np.mean(loss_list)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行召回  得到找回结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_result(model,data,embedding_dim,device,topN):\n",
    "    item_embs = model.output_items().cpu().detach().numpy()\n",
    "    \n",
    "    index = faiss.IndexFlatIP(embedding_dim)\n",
    "    index.add(item_embs)\n",
    "    \n",
    "    preds = defaultdict(list)\n",
    "    test_gd = defaultdict(list)\n",
    "    dummy_user_id = 0\n",
    "    \n",
    "    for (item_seq, mask, targets) in tqdm(data):\n",
    "        item_seq = item_seq.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        user_embs = model(item_seq,mask,None,train=False)['user_emb']\n",
    "        user_embs = user_embs.cpu().detach().numpy()\n",
    "        \n",
    "        num_interest = user_embs.shape[1]\n",
    "        user_embs = np.reshape(user_embs,[-1,embedding_dim]) # reshape之后 相连的 num_interest向量都是同一个user的\n",
    "        \n",
    "        D,I = index.search(user_embs,topN)\n",
    "        \n",
    "        for i,item_gd_list in enumerate(targets):\n",
    "            temp_df = pd.DataFrame()\n",
    "            temp_df['item'] = I[i*num_interest:(i+1)*num_interest].reshape(-1)\n",
    "            temp_df['score'] = D[i*num_interest:(i+1)*num_interest].reshape(-1)\n",
    "            temp_df = temp_df.sort_values(by='score',ascending=False)\n",
    "            temp_df = temp_df.drop_duplicates(subset=['item'], keep='first', inplace=False)\n",
    "            \n",
    "            recall_item_list = temp_df['item'][:topN].values\n",
    "            \n",
    "            preds[dummy_user_id] = list(recall_item_list)\n",
    "            test_gd[dummy_user_id] = item_gd_list\n",
    "            \n",
    "            dummy_user_id += 1\n",
    "            \n",
    "    return preds, test_gd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/54 [00:00<00:44,  1.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:29<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "preds, test_gd = get_recall_result(model,testloader,config['embedding_dim'],device,topN=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitrate(preds,test_gd):\n",
    "    hit_num = 0\n",
    "    total_num = 0\n",
    "    \n",
    "    for user in tqdm(preds.keys()):\n",
    "        hit_num += len(set(preds[user]) & set(test_gd[user]))\n",
    "        total_num += len(test_gd[user])\n",
    "    return hit_num / total_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算召回的100个物品的命中率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13600/13600 [00:00<00:00, 130977.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1949968109412971"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hitrate(preds,test_gd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 3.0468e-01,  3.2177e+00,  9.1495e-01,  ...,  3.1341e-01,\n",
       "          3.0869e-04,  3.7267e-01],\n",
       "        [ 1.3094e-01,  4.7279e-01,  1.5771e-01,  ..., -2.8948e-01,\n",
       "         -4.7956e-02, -1.1491e-01],\n",
       "        [-3.1506e-01, -4.9621e-01, -7.7427e-01,  ..., -1.4341e-02,\n",
       "         -3.3446e-01, -1.2319e-01],\n",
       "        ...,\n",
       "        [ 2.7224e-01,  3.1711e+00,  9.4916e-01,  ...,  2.8645e-01,\n",
       "          1.9601e-02,  3.3171e-01],\n",
       "        [ 3.0403e-01,  3.2188e+00,  9.1516e-01,  ...,  3.1349e-01,\n",
       "          1.2202e-03,  3.7368e-01],\n",
       "        [ 3.0533e-01,  3.2176e+00,  9.1509e-01,  ...,  3.1431e-01,\n",
       "         -1.4495e-05,  3.7236e-01]], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.item_emb.weight\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
